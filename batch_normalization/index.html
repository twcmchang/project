<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Chun-Ming Chang</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="description" content="Project - Chun Ming Chang">
    <link rel="shortcut icon" href="../favicon.ico">
    <link rel="stylesheet" href="../css/normalize.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="../css/cayman.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Why Batch Normalization Works so Well</h1>
      <h2 class="project-tagline">Machine Learning and having it Deep and Structured, NTU, 2017 Fall</h2>
      <a href="https://github.com/twcmchang/MLDS_final_2017" class="btn">View on GitHub</a>
      <a href="https://github.com/twcmchang/MLDS_final_2017/archive/master.zip" class="btn">Download .zip</a>
      <a href="https://twcmchang.github.io/online-cv/" class="btn">See my CV here</a>
    </section>

    <section class="main-content">

      <h2>
      <a id="user-content-header-2" class="anchor" href="#header-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>I. Motivation</h2>

      <p>Batch normalization is to accelerate network training by reducing internal covariate shift. <strong>Internal covariate shift</strong> is the change in the distribution of network activations due to the change in network parameters during training. By whitening the inputs to each layer, we would fix the input distributions and help remove the ill effects of the internal covariate shift.</p>

      <p>But whitening each layer’s inputs is costly and not everywhere differentiable, so batch normalization made two necessary simplifications:</p>
      <blockquote>
      <ol class="task-list">
      <li>Normalize each scalar feature <strong>independently</strong> instead of whitening</li>
      <li>Use <strong>mini-batches statistics</strong> rather than global statistics in stochastic gradient training</li>
      </ol>
      </blockquote>
      <p>Even though batch normalization (BN) is regarded as the necessary component in many well-known network architectures, but the question that why batch normalization works so well still remains mysterious. This work aims at investigating the possible reasons or explanation in both theoretical and experimental viewpoints.</p>

      <h2>
      <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>II. Experimental Study</h2>

      <p>We conduct experiments on MNIST using a fully connected neural network with 3 layers.</p>

      <h3>
      <a id="user-content-header-3" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>A. Mythbusters of Batch Normalization</h3>
      <p>Here we focus on validating the effects of batch normalization, and the author made the following claims in the original paper:
      </p>
      <blockquote>
      <p>Claim 1: BN regularizes the model</p>
      </blockquote>
      <p style="margin-left:22px "> Fig.1. implies that BN does limit the growth of magnitude of weights over training steps.</p>
      <blockquote>
      <p>Claim 2: BN solves the gradient vanishing problem</p>
      </blockquote>
      <p style="margin-left:22px"> Fig.2. shows that the average magnitude of gradients in the first hidden layer is about 0.1 with BN, but it drops to 0.02 if without BN.</p>
      <p><img src="images/bn_exp456.png" alt="" data-canonical-src="images/bn_exp456.png" style="max-width:100%;"></p>
      <blockquote>
      <p>Claim 3: BN benefits gradient flows through network</p>
      </blockquote>
      <p style="margin-left:22px">We restore the distribution of singular values of layer's Jacobian in every training steps. According to the following GIF(s), we found that BN helps shift the distribution of <strong>singular values of layer's Jacobian</strong> closer to one, and this keeps good <strong>isometry</strong> when error propagates through the network.</p>
      <img src="images/bn_l1_relu.gif" style="width:48%;" loop="100">
<img src="images/bn_l2_sigmoid.gif" style="width:48%;" loop="100">

      <h3>
      <a id="user-content-header-3" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>B. Which factors is the most influential to BN?</h3>

      <blockquote>
      <p>Activation functions: relu, lrelu, sigmoid, softplus, tanh</p>
      </blockquote>
      <p style="margin-left:22px "> In Fig.4., BN benefits the performance no matter which activation functions are used, and BN affects the case of Sigmoid most.</p>
      <blockquote>
      <p>Optimizers: SGD, Adam</p>
      </blockquote>
      <p style="margin-left:22px">Fig.5. shows that the selection of optimizers is less important than that of activation functions. (ReLU-SGD, with BN) is competitive to (ReLU-Adam, without BN), which reveals that BN is as effective as the sophisticated optimization algorithm like Adam.</p>
      <blockquote>
      <p>Batch size: 4, 16, 64, 256</p>
      </blockquote>
      <p style="margin-left:22px">Fig.5. depicts that BN deteriorates performance when batch size is small because the statistics of a small batch are not representive enough.</p>
      
      <p><img src="images/bn_exp123.png" alt="" data-canonical-src="images/bn_exp123.png" style="max-width:100%;"></p>

      <h2>
      <a id="user-content-header-2" class="anchor" href="#header-4" aria-hidden="true"><span class="octicon octicon-link"></span></a>III. Conclusions</h2>
      <ol class="task-list">
      <li> We have experimentally showed the facts
        <ul> 
        <li>BN improves performance no matter which activation functions or optimizers are selected; </li>
        <li>with BN, activation function is more crucial than optimizer</li>
        </ul>
      </li>
      <br>
      <li> We have verified the following claims
        <ul>
        <li> regularize the weights to reduce overfitting</li>
        <li> solve the issue of gradient vanishing significantly</li>
        <li> benefit gradient flow through network to accelerate training process</li>
        </ul>
        </li>
        <br>
      <li> By analyzing the distribution of singular values of layer's Jacobian, we show that BN is beneficial to keep better isometry while error backpropagtes through the network
      </li><br>
      <li> We investigate the conditions that BN worsens performance:
      <ul>
        <li> too small batch size </li>
        <li> mismatched distribution between training and testing datasets</li>
      </ul>
      But fortunately, the above issues can be solved by Batch Re-Nomalization (BRN)
      </li>
      </ol>
      <h2>
      <a id="user-content-header-2" class="anchor" href="#header-4" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h2>
      <ol>
      <li>Ioffe, Sergey, Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.</li>

      <li>Saxe, Andrew M., McClelland, James L., and Ganguli, Surya. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. CoRR, abs/1312.6120, 2013.</li>

      <li>Nair and Hinton, 2010 Nair, Vinod and Hinton, Geoffrey E. Rectified linear units improve restricted boltzmann machines. In ICML, pp. 807–814. Omnipress, 2010.</li>

      <li>Shimodaira, Hidetoshi. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of Statistical Planning and Inference, 90(2):227–244, October 2000.</li>

      <li>LeCun, Y., Bottou, L., Orr, G., and Muller, K. Efficient backprop. In Orr, G. and K., Muller (eds.), Neural Networks: Tricks of the trade. Springer, 1998b.</li>

      <li>Wiesler, Simon and Ney, Hermann. A convergence analysis of log-linear training. In Shawe-Taylor, J., Zemel, R.S., Bartlett, P., Pereira, F.C.N., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 24, pp. 657–665, Granada, Spain, December 2011.</li>

      <li>Wiesler, Simon, Richard, Alexander, Schlu ̈ter, Ralf, and Ney, Hermann. Mean-normalized stochastic gradient for large-scale deep learning. In IEEE International Conference on Acoustics, Speech, and Signal Processing, pp. 180–184, Florence, Italy, May 2014.

      <li>Raiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep learning made easier by linear transformations in perceptrons. In International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 924–932, 2012.</li>

      <li>Povey, Daniel, Zhang, Xiaohui, and Khudanpur, San- jeev. Parallel training of deep neural networks with natural gradient and parameter averaging. CoRR, abs/1410.7455, 2014.</li>

      <li>Wang, S., Mohamed, A. R., Caruana, R., Bilmes, J., Plilipose, M., Richardson, M., Aslan, O. (2016, June). Analysis of Deep Neural Networks with the Extended Data Jacobian Matrix. In Proceedings of The 33rd International Conference on Machine Learning (pp. 718-726).</li>

      <li>JIA, Kui. Improving training of deep neural networks via Singular Value Bounding. arXiv preprint arXiv:1611.06013, 2016.</li>

      <li>R2RT Implementing Batch Normalization in Tensorflow: https://r2rt.com/implementing-batch-normalization-in-tensorflow.html</li>
      </ol>

      <footer class="site-footer">
        <span class="site-footer-owner">© 2018 Chun Ming Chang. This theme is designed by <a href="https://github.com/jasonlong">jasonlong</a>.</span>
      </footer>

    </section>

  </body>
</html>
